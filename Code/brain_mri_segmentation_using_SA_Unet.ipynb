{"cells":[{"cell_type":"markdown","metadata":{"id":"anvuGthVzBai"},"source":["# **Brain MRI Segmentation using Spatial Attention UNet (SA-UNet)**\n","\n","This Jupyter Notebook details the implementation and evaluation of a specialized deep learning model, SA-UNet for the task of brain MRI segmentation. The objective is to accurately segment brain MRIs, which is crucial for medical diagnosis and treatment planning. The SA-UNet model leverages the U-Net architecture with an added spatial attention mechanism, enhancing its ability to focus on relevant features in MRI images. This notebook includes data preprocessing, model building, training, and evaluation steps, providing a comprehensive view of the project.\n","\n","## **Mounting Google Drive**\n","\n","Mount the Google Drive to load the dataset and any other resources stored in the drive. Navigate to the folder where dataset is stored."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19600,"status":"ok","timestamp":1706572616619,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"sToZpJDR5B64","outputId":"4c4cd9f1-321b-4f5f-ccce-250a83f77566"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1706572620541,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"MKJ82SWC5RVJ","outputId":"dcb8d44b-59e7-46ae-990f-8b3e0e88776c"},"outputs":[{"name":"stdout","output_type":"stream","text":["kaggle_3m  lgg-mri-segmentation\n"]}],"source":["!ls \"/content/drive/My Drive/archive\""]},{"cell_type":"markdown","metadata":{"id":"S53SsR2_hJx0"},"source":["## **Importing Necessary Libraries and Modules**\n","\n","In this section, we import all the necessary libraries and modules required for our brain MRI segmentation project."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2QOHTmLMDja"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.models import Model, load_model, save_model\n","from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, UpSampling2D, BatchNormalization, Activation, Dropout, Concatenate, Multiply, Lambda, concatenate\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"lpUucjOmh0To"},"source":["## **Understanding the Dataset**\n","\n","Here, we load the metadata from the data.csv file, which is an integral part of our brain MRI dataset. This file contains important information about the MRI images and their corresponding labels. Understanding the metadata is crucial for preprocessing and analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505,"status":"ok","timestamp":1706573607877,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"z6OnU7Av851i","outputId":"f3888635-2ff0-4d59-9553-102a551134e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["        Patient  RNASeqCluster  MethylationCluster  miRNACluster  CNCluster  \\\n","0  TCGA_CS_4941            2.0                 4.0             2        2.0   \n","1  TCGA_CS_4942            1.0                 5.0             2        1.0   \n","2  TCGA_CS_4943            1.0                 5.0             2        1.0   \n","3  TCGA_CS_4944            NaN                 5.0             2        1.0   \n","4  TCGA_CS_5393            4.0                 5.0             2        1.0   \n","\n","   RPPACluster  OncosignCluster  COCCluster  histological_type  \\\n","0          NaN              3.0           2                1.0   \n","1          1.0              2.0           1                1.0   \n","2          2.0              2.0           1                1.0   \n","3          2.0              1.0           1                1.0   \n","4          2.0              3.0           1                1.0   \n","\n","   neoplasm_histologic_grade  tumor_tissue_site  laterality  tumor_location  \\\n","0                        2.0                1.0         3.0             2.0   \n","1                        2.0                1.0         3.0             2.0   \n","2                        2.0                1.0         1.0             2.0   \n","3                        1.0                1.0         3.0             6.0   \n","4                        2.0                1.0         1.0             6.0   \n","\n","   gender  age_at_initial_pathologic  race  ethnicity  death01  \n","0     2.0                       67.0   3.0        2.0      1.0  \n","1     1.0                       44.0   2.0        NaN      1.0  \n","2     2.0                       37.0   3.0        NaN      0.0  \n","3     2.0                       50.0   3.0        NaN      0.0  \n","4     2.0                       39.0   3.0        NaN      0.0  \n"]}],"source":["# Load the data.csv to understand the metadata\n","data_csv_path = '/content/drive/My Drive/archive/lgg-mri-segmentation/kaggle_3m/data.csv'\n","data_df = pd.read_csv(data_csv_path)\n","\n","# Display the first few rows of the dataframe\n","print(data_df.head())"]},{"cell_type":"markdown","metadata":{"id":"XhcrVhrEijjt"},"source":["## **Data Visualization**\n","\n","Visualize the MRI images along with their corresponding masks. This step is crucial for understanding how the segmentation masks are applied to the brain MRI images. Visualization helps in assessing the quality of the data and the masks, which is essential for verifying the accuracy of our segmentation model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2794,"status":"ok","timestamp":1706573614394,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"VkxPYWdEAMRo","outputId":"02382755-3dc4-49e9-8625-3c5ec92cb6d3"},"outputs":[],"source":["def load_images_with_masks(base_dir, patient_id, num_samples=5):\n","    patient_path = os.path.join(base_dir, patient_id)\n","    image_files = [f for f in os.listdir(patient_path) if not f.endswith('_mask.tif')]\n","    image_files = sorted(image_files)[:num_samples]  # Limit the number of samples for visualization\n","\n","    images = []\n","    masks = []\n","\n","    for img_file in image_files:\n","        img_path = os.path.join(patient_path, img_file)\n","        mask_path = os.path.join(patient_path, img_file.replace('.tif', '_mask.tif'))\n","\n","        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n","        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n","\n","        if img is not None and mask is not None:\n","            images.append(img)\n","            masks.append(mask)\n","\n","    return images, masks\n","\n","def visualize_images_and_masks(images, masks):\n","    plt.figure(figsize=(10, 5 * len(images)))\n","    for i, (image, mask) in enumerate(zip(images, masks)):\n","        plt.subplot(len(images), 2, 2*i + 1)\n","        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        plt.title('Image')\n","        plt.axis('off')\n","\n","        plt.subplot(len(images), 2, 2*i + 2)\n","        plt.imshow(mask, cmap='gray')\n","        plt.title('Original_Mask')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","base_dir = '/content/drive/My Drive/archive/lgg-mri-segmentation/kaggle_3m/'\n","patient_id = 'TCGA_FG_A60K_20040224'\n","\n","images, masks = load_images_with_masks(base_dir, patient_id)\n","visualize_images_and_masks(images, masks)"]},{"cell_type":"markdown","metadata":{"id":"hrBQ4uAWi_I4"},"source":[]},{"cell_type":"markdown","metadata":{"id":"6nk2mXsKjNzO"},"source":["## **Counting the Total Number of Images in the Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1706573618132,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"QHy6sfFNB9sa","outputId":"712ba8a6-8c04-42d2-d66f-81f6ab7894ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of images: 3940\n","Total number of masks: 3929\n"]}],"source":["def count_total_images_and_masks(base_dir):\n","    total_images = 0\n","    total_masks = 0\n","\n","    for root, dirs, files in os.walk(base_dir):\n","        for file in files:\n","            if file.endswith('.tif') and '_mask' not in file:\n","                total_images += 1\n","            elif '_mask.tif' in file:\n","                total_masks += 1\n","\n","    return total_images, total_masks\n","\n","base_dir = '/content/drive/My Drive/archive/lgg-mri-segmentation/kaggle_3m/'\n","total_images, total_masks = count_total_images_and_masks(base_dir)\n","print(f\"Total number of images: {total_images}\")\n","print(f\"Total number of masks: {total_masks}\")"]},{"cell_type":"markdown","metadata":{"id":"TTNivO_kjSuW"},"source":["## **Data Preprocessing**\n","\n","This step involves dividing our dataset into three distinct sets: training (85%), validation (10%), and test (5%). This division is critical for effective model training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1577,"status":"ok","timestamp":1706573622065,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"nQNsYIxrCa6c","outputId":"6720829c-f144-4ff7-a6d2-6be9655c4de0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set: 3339 images\n","Validation set: 393 images\n","Test set: 197 images\n"]}],"source":["def get_image_mask_pairs(base_dir):\n","    all_images = []\n","    all_masks = []\n","\n","    for root, dirs, files in os.walk(base_dir):\n","        for file in sorted(files):\n","            if file.endswith('.tif') and not file.endswith('_mask.tif'):\n","                image_path = os.path.join(root, file)\n","                mask_path = os.path.join(root, file.replace('.tif', '_mask.tif'))\n","\n","                if os.path.exists(mask_path):  # Ensure the mask exists\n","                    all_images.append(image_path)\n","                    all_masks.append(mask_path)\n","\n","    return all_images, all_masks\n","\n","base_dir = '/content/drive/My Drive/archive/lgg-mri-segmentation/kaggle_3m/'\n","all_images, all_masks = get_image_mask_pairs(base_dir)\n","\n","# Initial split: 85% for training, 15% for temp (to be split further into validation and test)\n","train_images, temp_images, train_masks, temp_masks = train_test_split(all_images, all_masks, test_size=0.15, random_state=42)\n","\n","# Split the temp data into validation and test sets\n","# Since temp is 15% of the whole, validation is 10% of the whole (about 2/3 of temp), and test is 5% of the whole (about 1/3 of temp)\n","validation_images, test_images, validation_masks, test_masks = train_test_split(temp_images, temp_masks, test_size=1/3, random_state=42)\n","\n","print(f\"Training set: {len(train_images)} images\")\n","print(f\"Validation set: {len(validation_images)} images\")\n","print(f\"Test set: {len(test_images)} images\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6t9npeaGYK_"},"outputs":[],"source":["# Convert lists into DataFrames\n","df_train = pd.DataFrame({'image_path': train_images, 'mask_path': train_masks})\n","df_validation = pd.DataFrame({'image_path': validation_images, 'mask_path': validation_masks})\n","df_test = pd.DataFrame({'image_path': test_images, 'mask_path': test_masks})"]},{"cell_type":"markdown","metadata":{"id":"Q19chmnejnWs"},"source":["## **Setting Up the Training Data Generator**\n","\n","The generator applies real-time data augmentation techniques to the dataset during model training, including rotations, shifts, flips, and more. Such augmentations help in improving the model's robustness by simulating various conditions and orientations of MRI images. The generator also handles the efficient loading and processing of images in batches, which is crucial for training deep learning models on large datasets without overloading the memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wTV8_YeDoN2"},"outputs":[],"source":["def train_generator(\n","    data_frame,\n","    batch_size,\n","    augmentation_dict,\n","    image_color_mode=\"rgb\",\n","    mask_color_mode=\"grayscale\",\n","    image_save_prefix=\"image\",\n","    mask_save_prefix=\"mask\",\n","    save_to_dir=None,\n","    target_size=(256, 256),\n","    seed=1,\n","):\n","    image_datagen = ImageDataGenerator(**augmentation_dict)\n","    mask_datagen = ImageDataGenerator(**augmentation_dict)\n","\n","    # Create generators for images and masks\n","    image_generator = image_datagen.flow_from_dataframe(\n","        data_frame,\n","        x_col=\"image_path\",\n","        class_mode=None,\n","        color_mode=image_color_mode,\n","        target_size=target_size,\n","        batch_size=batch_size,\n","        save_to_dir=save_to_dir,\n","        save_prefix=image_save_prefix,\n","        seed=seed,\n","    )\n","\n","    mask_generator = mask_datagen.flow_from_dataframe(\n","        data_frame,\n","        x_col=\"mask_path\",\n","        class_mode=None,\n","        color_mode=mask_color_mode,\n","        target_size=target_size,\n","        batch_size=batch_size,\n","        save_to_dir=save_to_dir,\n","        save_prefix=mask_save_prefix,\n","        seed=seed,\n","    )\n","\n","    train_gen = zip(image_generator, mask_generator)\n","\n","    for (img, mask) in train_gen:\n","        img, mask = normalize_data(img, mask)\n","        yield (img, mask)"]},{"cell_type":"markdown","metadata":{"id":"hn2uc2SdkE5W"},"source":["## **Data Normalization**\n","\n","Normalization is a crucial preprocessing step in deep learning as it scales the pixel intensity values to a standard range, typically between 0 and 1. This process helps in stabilizing the training process and improving model convergence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhRDKZw3kEJ0"},"outputs":[],"source":["def normalize_data(img, mask):\n","    img = img / 255.0\n","    mask = mask / 255.0\n","    mask[mask > 0.5] = 1\n","    mask[mask <= 0.5] = 0\n","    return (img, mask)"]},{"cell_type":"markdown","metadata":{"id":"tDy-cPKnkkGV"},"source":["## **Loss Function and Evaluation Metrics**\n","\n","Here we define the loss function and metrics that will be used to train and evaluate our model.\n","\n","**Dice Coefficient Loss:** A custom loss function based on the Dice coefficient, which is effective for segmentation tasks as it measures the overlap between the predicted and actual masks.\n","\n","**Evaluation Metrics:** These include accuracy, Intersection over Union (IoU), and Dice coefficient. These metrics provide insights into various aspects of the model's performance, from general accuracy to specific overlap between the predicted and true segmentation areas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6zFgI90D60j"},"outputs":[],"source":["def dice_coefficients(y_true, y_pred, smooth=100):\n","    y_true_flatten = K.flatten(y_true)\n","    y_pred_flatten = K.flatten(y_pred)\n","\n","    intersection = K.sum(y_true_flatten * y_pred_flatten)\n","    union = K.sum(y_true_flatten) + K.sum(y_pred_flatten)\n","    return (2 * intersection + smooth) / (union + smooth)\n","\n","def dice_coefficients_loss(y_true, y_pred, smooth=100):\n","    return -dice_coefficients(y_true, y_pred, smooth)\n","def iou(y_true, y_pred, smooth=100):\n","    intersection = K.sum(y_true * y_pred)\n","    sum = K.sum(y_true + y_pred)\n","    iou = (intersection + smooth) / (sum - intersection + smooth)\n","    return iou\n","def jaccard_distance(y_true, y_pred):\n","    y_true_flatten = K.flatten(y_true)\n","    y_pred_flatten = K.flatten(y_pred)\n","    return -iou(y_true_flatten, y_pred_flatten)"]},{"cell_type":"markdown","metadata":{"id":"_qB2FV3zk02-"},"source":["## **Defining Encoder and Decoder Blocks for the SA-UNet Model**\n","\n","**Encoder Blocks:** Each block comprises convolutional layers followed by dropout, batch normalization, relu and max pooling for downsampling.\n","\n","**Spatial Attention block:** It applies both max pooling and average pooling across the channel dimension to capture different aspects of the spatial features, concatenate their outputs, normalize by sigmoid activation and multiply with input feature map to generate attention map.\n","\n","**Decoder Blocks:** Each decoder block starts with a transposed convolution for upsampling, followed by concatenation with the corresponding encoder output (skip connection), and convolutional layers similar to those in the encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gt0D-60tlyIB"},"outputs":[],"source":["def encoder_block(input_tensor, num_filters, dropout_rate=0.2):\n","    x = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n","    x = Dropout(dropout_rate)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    p = MaxPooling2D((2, 2))(x)\n","    return x, p\n","\n","def spatial_attention_block(input_tensor):\n","    max_pool = tf.reduce_max(input_tensor, axis=-1, keepdims=True)\n","    avg_pool = tf.reduce_mean(input_tensor, axis=-1, keepdims=True)\n","\n","    concat = tf.concat([max_pool, avg_pool], axis=-1)\n","    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)\n","    output = Multiply()([input_tensor, attention])\n","\n","    return output\n","\n","def decoder_block(input_tensor, skip_features, num_filters, dropout_rate=0.2):\n","    x = Conv2DTranspose(num_filters, (3, 3), strides=(2, 2), padding='same')(input_tensor)\n","    x = concatenate([x, skip_features])\n","\n","    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"UkjOXaV5vpoC"},"source":["## **Defining the  Spatial Attention U-Net Architecture**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAzA3GxpnMnV"},"outputs":[],"source":["img_width, img_height = 256, 256\n","def unet_model(input_shape = (img_width, img_height, 3)):\n","  inputs = Input(input_shape)\n","\n","  # Encoder\n","  e1, p1 = encoder_block(inputs, 16, 0.2)\n","  e2, p2 = encoder_block(p1, 32, 0.2)\n","  e3, p3 = encoder_block(p2, 64, 0.2)\n","  e4, p4 = encoder_block(p3, 128, 0.2)\n","\n","  # Spatial Attention\n","  sa = spatial_attention_block(p4)\n","\n","  # Decoder\n","  d1 = decoder_block(sa, e4, 128, 0.2)\n","  d2 = decoder_block(d1, e3, 64, 0.2)\n","  d3 = decoder_block(d2, e2, 32, 0.2)\n","  d4 = decoder_block(d3, e1, 16, 0.2)\n","\n","  # Output\n","  outputs = Conv2D(filters=1, kernel_size =(1, 1), activation='sigmoid')(d4)\n","\n","  return Model(inputs=[inputs], outputs = [outputs])"]},{"cell_type":"markdown","metadata":{"id":"VWc8v-nCwRol"},"source":["## **Setting Up Training Parameters and Data Augmentation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6069274,"status":"ok","timestamp":1706568932157,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"UNtfPdexH_fX","outputId":"17a802ee-14c7-40ca-a7fc-3046be94b5fb"},"outputs":[],"source":["epochs = 50\n","batch_size = 32\n","lr = 1e-4\n","\n","train_generator_args = dict(\n","    rotation_range = 0.25,\n","    width_shift_range = 0.05,\n","    height_shift_range=0.05,\n","    shear_range=0.05,\n","    zoom_range=0.05,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","train_gen  = train_generator( df_train, batch_size, train_generator_args)\n","test_gen = train_generator(df_test, batch_size, dict() )\n","\n","model = unet_model(input_shape =(256,256, 3))\n","optimizer = Adam(learning_rate = lr , beta_1 = 0.9, beta_2 = 0.999, epsilon = None, amsgrad = False)\n","\n","model.compile(optimizer=optimizer, loss=dice_coefficients_loss, metrics=[\"binary_accuracy\", iou, dice_coefficients])\n","callbacks = [\n","    ModelCheckpoint('unet_model.hdf5',verbose=1, save_best_only=True )\n","]\n","\n","history = model.fit(\n","      train_gen,\n","      steps_per_epoch = len(df_train) / batch_size,\n","      epochs = epochs,\n","      callbacks = callbacks,\n","      validation_data = test_gen,\n","      validation_steps = len(df_validation) / batch_size\n",")"]},{"cell_type":"markdown","metadata":{"id":"C2T-SgCXwv7T"},"source":["## **Visualize the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30GH8hgRQpVx"},"outputs":[],"source":["from tensorflow import keras\n","keras.utils.plot_model(model, to_file = \"Model.png\", show_shapes=True)"]},{"cell_type":"markdown","metadata":{"id":"aaIwp5QGxLh3"},"source":["## **Plotting Loss and Accuracy Graphs**\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":945},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1706571883130,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"VAP7UJVPQyYi","outputId":"542d2002-71b2-4972-e3e7-bdff5968f003"},"outputs":[],"source":["history_training = history.history\n","\n","train_dice_coeff_list = history_training['dice_coefficients']\n","test_dice_coeff_list = history_training['val_dice_coefficients']\n","\n","train_jaccard_list = history_training['iou']\n","test_jaccard_list = history_training['val_iou']\n","\n","train_loss_list = history_training['loss']\n","test_loss_list = history_training['val_loss']\n","\n","\n","plt.plot(test_loss_list, 'b-', label='Test Loss')\n","plt.plot(train_loss_list, 'r-', label='Train Loss')\n","\n","plt.xlabel('iterations')\n","plt.ylabel('loss')\n","plt.title('loss graph', fontsize=12)\n","plt.legend()\n","plt.show()\n","plt.savefig('Loss Graph')\n","\n","plt.plot(train_dice_coeff_list, 'b-', label='Train Accuracy')\n","plt.plot(test_dice_coeff_list, 'r-', label = 'Test Accuracy')\n","\n","plt.xlabel('iterations')\n","plt.ylabel('accuracy')\n","plt.title('Accuracy graph', fontsize=12)\n","plt.legend()\n","plt.show()\n","plt.savefig('Accuracy Graph')"]},{"cell_type":"markdown","metadata":{"id":"BbY7LZbPyBu7"},"source":["## **Load Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4AZbFVhQ6oj"},"outputs":[],"source":["model = load_model('unet_model.hdf5', custom_objects={'dice_coefficients_loss': dice_coefficients_loss, 'iou': iou, 'dice_coefficients': dice_coefficients})"]},{"cell_type":"markdown","metadata":{"id":"m2i6WSagyWEx"},"source":["## **Model Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3076,"status":"ok","timestamp":1706571810867,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"w4doXVqbQ9iY","outputId":"40a053d1-0a92-43e9-ddda-fd0b7b7be056"},"outputs":[],"source":["test_gen = train_generator(df_test, batch_size, dict(), target_size=(img_height,img_width))\n","results = model.evaluate(test_gen, steps = len(df_test)/batch_size)\n","print('Test Loss', results[0])\n","print('Test IOU', results[1])\n","print('Test Dice Coeff', results[2])"]},{"cell_type":"markdown","metadata":{"id":"xxdQPP-yyLob"},"source":["## **Visualize Predictions**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fMVH0qD24sq_GDSOqR1K0cWogzUSQbW2"},"executionInfo":{"elapsed":64918,"status":"ok","timestamp":1706571882609,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"},"user_tz":480},"id":"XiTjmAkAe54O","outputId":"583f1e30-8145-4996-e717-693528417c30"},"outputs":[],"source":["for i in range(20):\n","    index = np.random.randint(0, len(df_test.index))\n","    img_path = df_test['image_path'].iloc[index]\n","    mask_path = df_test['mask_path'].iloc[index]\n","\n","    # Read and preprocess the image\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img_resized = cv2.resize(img, (256, 256))\n","    img_normalized = img_resized / 255.0\n","    img_expanded = np.expand_dims(img_normalized, axis=0)\n","\n","    # Generate the prediction\n","    pred_mask = model.predict(img_expanded)\n","    pred_mask_thresholded = (pred_mask.squeeze() > 0.5).astype(np.float32)\n","\n","    # Read and preprocess the mask\n","    original_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","    original_mask_resized = cv2.resize(original_mask, (256, 256))\n","    fig, axs = plt.subplots(1, 3, figsize=(18, 6))  # Width, Height\n","\n","    # Display the original image\n","    axs[0].imshow(img_resized)\n","    axs[0].set_title(\"Original Image\")\n","    axs[0].axis('off')  # Hide axes ticks\n","\n","    # Display the original mask\n","    axs[1].imshow(original_mask_resized, cmap='gray')\n","    axs[1].set_title(\"Original Mask\")\n","    axs[1].axis('off')\n","\n","    # Display the predicted mask\n","    axs[2].imshow(pred_mask_thresholded, cmap='gray')\n","    axs[2].set_title(\"Predicted Mask\")\n","    axs[2].axis('off')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pldouRdOR3dz"},"outputs":[],"source":["hist_df = pd.DataFrame(history.history)\n","\n","# save to json:\n","hist_json_file = 'history.json'\n","with open(hist_json_file, mode='w') as f:\n","    hist_df.to_json(f)\n","\n","# or save to csv:\n","hist_csv_file = 'history.csv'\n","with open(hist_csv_file, mode='w') as f:\n","    hist_df.to_csv(f)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPWbhEQtZ0UdCoBcaE6KOTD","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
